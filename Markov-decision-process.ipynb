{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "33f6a30f",
   "metadata": {},
   "source": [
    "You will need the following values for the implementation:\n",
    "1. Reward for reaching the goal state = 1\n",
    "2. Penalty for reaching the red state = -1\n",
    "3. Step cost = -0.04\n",
    "4. Probability of going in the direction of the action = p\n",
    "5. Probability of going in a direction perpendicular to the action = (1−p)/2.\n",
    "6. Discount factor = 0.95\n",
    "One thing to note is that if the agent is unable to move according to the action i.e., the move is blocked by the boundaries or the wall, the agent will remain in the same state.\n",
    "\n",
    "Task A-\n",
    "Task is to implement Value Iteration in Python for the given grid world (Fig 1). Take p = 0.7 for this task. The code should print the utility value of each cell in the grid after each iteration until the values converge, where convergence is defined by a difference ≤ 0.0001 between utility values.\n",
    "![fig.png](/home/navdha/gitrepo/Machine-data-and-Learning-concepts/fig.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aba76aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1:\n",
      "Utility Grid:\n",
      "-0.040000\t-1.000000\t1.000000\n",
      "-0.040000\t-0.040000\t0.625000\n",
      "-0.040000\t0.000000\t-0.040000\n",
      "-0.040000\t-0.040000\t-0.040000\n",
      "\n",
      "Iteration 2:\n",
      "Utility Grid:\n",
      "-0.078000\t-1.000000\t1.000000\n",
      "-0.078000\t0.227425\t0.708362\n",
      "-0.078000\t0.000000\t0.364225\n",
      "-0.078000\t-0.078000\t-0.078000\n",
      "\n",
      "Iteration 3:\n",
      "Utility Grid:\n",
      "-0.114100\t-1.000000\t1.000000\n",
      "0.089008\t0.320969\t0.758350\n",
      "-0.114100\t0.000000\t0.534865\n",
      "-0.114100\t-0.114100\t0.179980\n",
      "\n",
      "Iteration 4:\n",
      "Utility Grid:\n",
      "-0.119452\t-1.000000\t1.000000\n",
      "0.140926\t0.367541\t0.778803\n",
      "-0.013328\t0.000000\t0.616739\n",
      "-0.148395\t0.047168\t0.325073\n",
      "\n",
      "Iteration 5:\n",
      "Utility Grid:\n",
      "-0.105806\t-1.000000\t1.000000\n",
      "0.185493\t0.387778\t0.788354\n",
      "0.049917\t0.000000\t0.653675\n",
      "-0.031679\t0.189617\t0.423176\n",
      "\n",
      "Iteration 6:\n",
      "Utility Grid:\n",
      "-0.074224\t-1.000000\t1.000000\n",
      "0.209909\t0.397014\t0.792599\n",
      "0.097579\t0.000000\t0.670553\n",
      "0.088694\t0.295453\t0.482017\n",
      "\n",
      "Iteration 7:\n",
      "Utility Grid:\n",
      "-0.053488\t-1.000000\t1.000000\n",
      "0.227342\t0.401153\t0.794520\n",
      "0.127399\t0.000000\t0.678186\n",
      "0.183020\t0.364745\t0.516707\n",
      "\n",
      "Iteration 8:\n",
      "Utility Grid:\n",
      "-0.038939\t-1.000000\t1.000000\n",
      "0.237299\t0.403020\t0.795383\n",
      "0.147491\t0.000000\t0.681639\n",
      "0.246790\t0.407562\t0.536600\n",
      "\n",
      "Iteration 9:\n",
      "Utility Grid:\n",
      "-0.030245\t-1.000000\t1.000000\n",
      "0.243477\t0.403860\t0.795772\n",
      "0.166151\t0.000000\t0.683197\n",
      "0.287214\t0.432995\t0.547833\n",
      "\n",
      "Iteration 10:\n",
      "Utility Grid:\n",
      "-0.024898\t-1.000000\t1.000000\n",
      "0.247934\t0.404239\t0.795948\n",
      "0.198350\t0.000000\t0.683900\n",
      "0.312546\t0.447712\t0.554094\n",
      "\n",
      "Iteration 11:\n",
      "Utility Grid:\n",
      "-0.021172\t-1.000000\t1.000000\n",
      "0.253536\t0.404409\t0.796027\n",
      "0.224373\t0.000000\t0.684217\n",
      "0.330531\t0.456070\t0.557551\n",
      "\n",
      "Iteration 12:\n",
      "Utility Grid:\n",
      "-0.016916\t-1.000000\t1.000000\n",
      "0.257888\t0.404486\t0.796062\n",
      "0.243750\t0.000000\t0.684359\n",
      "0.342361\t0.460751\t0.559445\n",
      "\n",
      "Iteration 13:\n",
      "Utility Grid:\n",
      "-0.013415\t-1.000000\t1.000000\n",
      "0.261307\t0.404521\t0.796078\n",
      "0.257139\t0.000000\t0.684424\n",
      "0.349920\t0.463345\t0.560477\n",
      "\n",
      "Iteration 14:\n",
      "Utility Grid:\n",
      "-0.010642\t-1.000000\t1.000000\n",
      "0.263737\t0.404536\t0.796085\n",
      "0.265982\t0.000000\t0.684453\n",
      "0.354630\t0.464771\t0.561036\n",
      "\n",
      "Iteration 15:\n",
      "Utility Grid:\n",
      "-0.008632\t-1.000000\t1.000000\n",
      "0.265402\t0.404543\t0.796089\n",
      "0.271634\t0.000000\t0.684466\n",
      "0.357510\t0.465549\t0.561339\n",
      "\n",
      "Iteration 16:\n",
      "Utility Grid:\n",
      "-0.007237\t-1.000000\t1.000000\n",
      "0.266499\t0.404546\t0.796090\n",
      "0.275160\t0.000000\t0.684472\n",
      "0.359243\t0.465972\t0.561501\n",
      "\n",
      "Iteration 17:\n",
      "Utility Grid:\n",
      "-0.006309\t-1.000000\t1.000000\n",
      "0.267202\t0.404548\t0.796091\n",
      "0.277317\t0.000000\t0.684474\n",
      "0.360273\t0.466200\t0.561588\n",
      "\n",
      "Iteration 18:\n",
      "Utility Grid:\n",
      "-0.005710\t-1.000000\t1.000000\n",
      "0.267643\t0.404548\t0.796091\n",
      "0.278617\t0.000000\t0.684475\n",
      "0.360880\t0.466323\t0.561635\n",
      "\n",
      "Iteration 19:\n",
      "Utility Grid:\n",
      "-0.005331\t-1.000000\t1.000000\n",
      "0.267914\t0.404549\t0.796091\n",
      "0.279391\t0.000000\t0.684476\n",
      "0.361233\t0.466390\t0.561660\n",
      "\n",
      "Iteration 20:\n",
      "Utility Grid:\n",
      "-0.005097\t-1.000000\t1.000000\n",
      "0.268078\t0.404549\t0.796091\n",
      "0.279847\t0.000000\t0.684476\n",
      "0.361438\t0.466425\t0.561674\n",
      "\n",
      "Iteration 21:\n",
      "Utility Grid:\n",
      "-0.004954\t-1.000000\t1.000000\n",
      "0.268177\t0.404549\t0.796091\n",
      "0.280113\t0.000000\t0.684476\n",
      "0.361556\t0.466444\t0.561681\n",
      "\n",
      "Iteration 22:\n",
      "Utility Grid:\n",
      "-0.004868\t-1.000000\t1.000000\n",
      "0.268235\t0.404549\t0.796091\n",
      "0.280267\t0.000000\t0.684476\n",
      "0.361623\t0.466454\t0.561685\n",
      "\n",
      "Iteration 23:\n",
      "Utility Grid:\n",
      "-0.004817\t-1.000000\t1.000000\n",
      "0.268269\t0.404549\t0.796091\n",
      "0.280355\t0.000000\t0.684476\n",
      "0.361661\t0.466460\t0.561687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorldMDP:\n",
    "    def __init__(self):\n",
    "        self.grid_world = np.array([[0, -1, 1], [0, 0, 0], [0, None, 0], [0, 0, 0]])\n",
    "        self.possible_actions = [(0,1), (0, -1), (1, 0), (-1, 0)]\n",
    "        self.reward = 1\n",
    "        self.penalty = -1\n",
    "        self.step_cost = -0.04\n",
    "        self.discount_factor = 0.95\n",
    "        self.probability_selected_action = 0.7\n",
    "        self.probability_other_actions = 0.15\n",
    "        self.U = np.zeros((4,3))\n",
    "        self.U[0][1] = -1\n",
    "        self.U[0][2] = 1\n",
    "        self.policy = np.zeros((4,3))\n",
    "\n",
    "    def value(self, row, col, action):\n",
    "     total_value = 0\n",
    "    \n",
    "     for i in [-1, 0, 1]:\n",
    "      for j in [-1, 0, 1]:\n",
    "            if (i, j) == (-action[0], -action[1]):\n",
    "                continue\n",
    "            if (i == 0 and abs(j) == 1) or (j == 0 and abs(i) == 1):\n",
    "                current_prob = self.probability_selected_action if (i, j) == action else self.probability_other_actions\n",
    "                new_row, new_col = row + i, col + j\n",
    "                \n",
    "                if 0 <= new_row < 4 and 0 <= new_col < 3 and self.grid_world[new_row][new_col] is not None:\n",
    "                    total_value += current_prob * self.U[new_row, new_col]\n",
    "                else:\n",
    "                    total_value += current_prob * self.U[row, col]\n",
    "    \n",
    "     return total_value\n",
    "\n",
    "\n",
    "    def iterate_values(self):\n",
    "     iteration = 1\n",
    "     while True:\n",
    "        delta = 0\n",
    "        U1 = np.zeros((4, 3))\n",
    "        \n",
    "        for row in range(4):\n",
    "            for col in range(3):\n",
    "                old_value = self.U[row, col]\n",
    "                \n",
    "                if self.grid_world[row][col] is None:\n",
    "                    self.policy[row][col] = -1 \n",
    "                    continue\n",
    "                \n",
    "                if self.grid_world[row][col] == 1:\n",
    "                    U1[row][col] = self.reward\n",
    "                    self.policy[row][col] = -1  \n",
    "                \n",
    "                elif self.grid_world[row][col] == -1:\n",
    "                    U1[row][col] = self.penalty\n",
    "                    self.policy[row][col] = -1 \n",
    "                \n",
    "                else:\n",
    "                  best_action_index = 0\n",
    "                  best_action_value = self.value(row, col, self.possible_actions[0])\n",
    "    \n",
    "                  for i in range(1, len(self.possible_actions)):\n",
    "                    action_value = self.value(row, col, self.possible_actions[i])\n",
    "                    if action_value > best_action_value:\n",
    "                       best_action_value = action_value\n",
    "                       best_action_index = i\n",
    "    \n",
    "                  U1[row, col] = self.step_cost + (self.discount_factor * best_action_value)\n",
    "                  self.policy[row, col] = best_action_index  \n",
    "\n",
    "                delta = abs(old_value - U1[row, col]) if abs(old_value - U1[row, col]) > delta else delta\n",
    "\n",
    "        \n",
    "        self.U = U1\n",
    "        print(f\"Iteration {iteration}:\")\n",
    "        self.print_values()\n",
    "        iteration += 1\n",
    "        \n",
    "        if delta <= 0.0001:\n",
    "            break\n",
    "\n",
    "    def print_values(self):\n",
    "     print('Utility Grid:')\n",
    "     for row in self.U:\n",
    "        print(\"\\t\".join(\"{:.6f}\".format(value) for value in row))\n",
    "     print()\n",
    "\n",
    "    def print_policy(self):\n",
    "     action_symbols = {0: 'right', 1: 'left', 2: 'down', 3: 'up', -1: 'none'}\n",
    "     print('Optimal policy grid:')\n",
    "     for row in self.policy:\n",
    "        print(\" \".join(\"{:^6}\".format(action_symbols[int(action)]) for action in row))\n",
    "\n",
    "# Usage\n",
    "mdp = GridWorldMDP()\n",
    "mdp.iterate_values()\n",
    "#mdp.print_policy()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "caf1c860",
   "metadata": {},
   "source": [
    "Task B-\n",
    "Run the algorithm for p values ranging from 0.1 to 0.9 with steps of 0.1. Print only the final policies that\n",
    "the algorithm converges to in the following format for each p.\n",
    "’right’ ’none’ ’none’\n",
    "’up’ ’up’ ’down’\n",
    "’up’ ’none’ ’down’\n",
    "’up’ ’left’ ’left’\n",
    "Observe how the policy changes with different p values. Comment on why the changes occur as they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "884da787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal policy grid (p=0.1):\n",
      " left   none   none \n",
      "  up    down  right \n",
      "right   none  right \n",
      "  up    down  right \n",
      "Optimal policy grid (p=0.2):\n",
      " left   none   none \n",
      "  up    down  right \n",
      "right   none  right \n",
      "  up    down  right \n",
      "Optimal policy grid (p=0.30000000000000004):\n",
      " left   none   none \n",
      " down   down  right \n",
      "right   none    up  \n",
      " down  right  right \n",
      "Optimal policy grid (p=0.4):\n",
      " left   none   none \n",
      " down   down  right \n",
      "right   none    up  \n",
      " down  right  right \n",
      "Optimal policy grid (p=0.5):\n",
      " left   none   none \n",
      "right   down    up  \n",
      " down   none    up  \n",
      "right  right    up  \n",
      "Optimal policy grid (p=0.6):\n",
      " left   none   none \n",
      "right   down    up  \n",
      " down   none    up  \n",
      "right  right    up  \n",
      "Optimal policy grid (p=0.7000000000000001):\n",
      " down   none   none \n",
      "right  right    up  \n",
      " down   none    up  \n",
      "right  right    up  \n",
      "Optimal policy grid (p=0.8):\n",
      " down   none   none \n",
      "right  right    up  \n",
      " down   none    up  \n",
      "right  right    up  \n",
      "Optimal policy grid (p=0.9):\n",
      " down   none   none \n",
      "right  right    up  \n",
      "  up    none    up  \n",
      "right  right    up  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class GridWorldMDP:\n",
    "    def __init__(self, probability_selected_action):\n",
    "        self.grid_world = np.array([[0, -1, 1], [0, 0, 0], [0, None, 0], [0, 0, 0]])\n",
    "        self.possible_actions = [(0,1), (0, -1), (1, 0), (-1, 0)]\n",
    "        self.reward = 1\n",
    "        self.penalty = -1\n",
    "        self.step_cost = -0.04\n",
    "        self.discount_factor = 0.95\n",
    "        self.probability_selected_action = probability_selected_action\n",
    "        self.probability_other_actions = (1 - probability_selected_action) / 2\n",
    "        self.U = np.zeros((4,3))\n",
    "        self.U[0][1] = -1\n",
    "        self.U[0][2] = 1\n",
    "        self.policy = np.zeros((4,3))\n",
    "\n",
    "    def value(self, row, col, action):\n",
    "        total_value = 0\n",
    "    \n",
    "        for i in [-1, 0, 1]:\n",
    "            for j in [-1, 0, 1]:\n",
    "                if (i, j) == (-action[0], -action[1]):\n",
    "                    continue\n",
    "                if (i == 0 and abs(j) == 1) or (j == 0 and abs(i) == 1):\n",
    "                    current_prob = self.probability_selected_action if (i, j) == action else self.probability_other_actions\n",
    "                    new_row, new_col = row + i, col + j\n",
    "                    \n",
    "                    if 0 <= new_row < 4 and 0 <= new_col < 3 and self.grid_world[new_row][new_col] is not None:\n",
    "                        total_value += current_prob * self.U[new_row, new_col]\n",
    "                    else:\n",
    "                        total_value += current_prob * self.U[row, col]\n",
    "        \n",
    "        return total_value\n",
    "\n",
    "    def iterate_values(self):\n",
    "        iteration = 1\n",
    "        while True:\n",
    "            delta = 0\n",
    "            U1 = np.zeros((4, 3))\n",
    "            \n",
    "            for row in range(4):\n",
    "                for col in range(3):\n",
    "                    old_value = self.U[row, col]\n",
    "                    \n",
    "                    if self.grid_world[row][col] is None:\n",
    "                        self.policy[row][col] = -1  \n",
    "                        continue\n",
    "                    \n",
    "                    if self.grid_world[row][col] == 1:\n",
    "                        U1[row][col] = self.reward\n",
    "                        self.policy[row][col] = -1  \n",
    "                    \n",
    "                    elif self.grid_world[row][col] == -1:\n",
    "                        U1[row][col] = self.penalty\n",
    "                        self.policy[row][col] = -1 \n",
    "                    else:\n",
    "                        best_action_index = 0\n",
    "                        best_action_value = self.value(row, col, self.possible_actions[0])\n",
    "    \n",
    "                        for i in range(1, len(self.possible_actions)):\n",
    "                          action_value = self.value(row, col, self.possible_actions[i])\n",
    "                          if action_value > best_action_value:\n",
    "                            best_action_value = action_value\n",
    "                            best_action_index = i\n",
    "    \n",
    "                        U1[row, col] = self.step_cost + (self.discount_factor * best_action_value)\n",
    "                        self.policy[row, col] = best_action_index  \n",
    "                        \n",
    "                    delta = abs(old_value - U1[row, col]) if abs(old_value - U1[row, col]) > delta else delta\n",
    "                    \n",
    "            \n",
    "            self.U = U1\n",
    "            iteration += 1\n",
    "            \n",
    "            if delta <= 0.0001:\n",
    "                break\n",
    "\n",
    "    def print_policy(self):\n",
    "        action_symbols = {0: 'right', 1: 'left', 2: 'down', 3: 'up', -1: 'none'}\n",
    "        print('Optimal policy grid (p={}):'.format(self.probability_selected_action))\n",
    "        for row in self.policy:\n",
    "            print(\" \".join(\"{:^6}\".format(action_symbols[int(action)]) for action in row))\n",
    "\n",
    "# Usage\n",
    "for p in np.arange(0.1, 1.0, 0.1):\n",
    "    mdp = GridWorldMDP(probability_selected_action=p)\n",
    "    mdp.iterate_values()\n",
    "    mdp.print_policy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188eb649",
   "metadata": {},
   "source": [
    " let's analyze how the policy changes with different values of pp, the probability of selecting the desired action.\n",
    "\n",
    "    1.p=0.1:\n",
    "        The optimal policy mostly prefers moving right or up. This might be because with a low probability of selecting the desired action, the agent tends to prefer safer moves (i.e., moves that are less likely to lead to a negative reward).\n",
    "\n",
    "    2.p=0.2 to p=0.5:\n",
    "        The optimal policy remains similar across these probabilities, favoring moves to the right and down when possible. This could be because with a moderate probability of selecting the desired action, the agent is more willing to take risks and explore potential high-reward actions.\n",
    "\n",
    "    3.p=0.6 to p=0.9:\n",
    "        The optimal policy starts favoring moves downward more frequently. This might be because with a higher probability of selecting the desired action, the agent becomes more confident in its ability to reach the desired state, allowing it to prioritize downward moves which might lead to higher rewards.\n",
    "\n",
    "In general, the changes in the optimal policy as pp varies are influenced by the agent's risk-taking behavior. Lower values of pp lead to safer policies, while higher values encourage riskier actions, assuming that the agent trusts its ability to select the desired action. This demonstrates the trade-off between exploration and exploitation in reinforcement learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
